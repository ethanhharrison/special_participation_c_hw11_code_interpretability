{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c92cd5",
   "metadata": {},
   "source": [
    "# **Transformer Interpretability**\n",
    "\n",
    "In this coding homework, you will:\n",
    "* Implement a single attention head\n",
    "* Implement an induction copy head by combining a previous token head with a copying head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the github repo\n",
    "# !git clone https://github.com/Berkeley-CS182/cs182fa25_public.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca910b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd cs182fa25_public/hw11/code/q_coding_interpretability/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae2eee",
   "metadata": {},
   "source": [
    "## **Imports & Preliminaries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "\n",
    "SEED = 2025\n",
    "EPS = 1e-6\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afa72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_heatmap(\n",
    "    matrix: ArrayLike,\n",
    "    title: str = \"Weight Matrix Heatmap\",\n",
    "    cmap: str = \"RdBu_r\",\n",
    "    annotate: bool = True,\n",
    "    figsize: Tuple[int, int] = (8, 6),\n",
    "    precision: int = 2,\n",
    ") -> Tuple[plt.Figure, plt.Axes]:\n",
    "    \"\"\"\n",
    "    Plot the weights of a matrix as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        matrix: 2D array-like - The weight matrix to visualize\n",
    "        title: Title for the plot\n",
    "        cmap: Colormap to use (default: 'RdBu_r' for diverging red-blue)\n",
    "        annotate: Whether to show values in each cell\n",
    "        figsize: Figure size (width, height)\n",
    "        precision: Number of decimal places for annotations\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (fig, ax) - Matplotlib figure and axes objects\n",
    "    \"\"\"\n",
    "    matrix = np.asarray(matrix)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create heatmap with symmetric color scale around zero\n",
    "    vmax = np.abs(matrix).max()\n",
    "    vmin = -vmax if vmax != 0 else -1\n",
    "\n",
    "    im = ax.imshow(matrix, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Weight Value')\n",
    "\n",
    "    # Add annotations if requested and matrix is small enough\n",
    "    if annotate and matrix.size <= 100:\n",
    "        for i in range(matrix.shape[0]):\n",
    "            for j in range(matrix.shape[1]):\n",
    "                text_color = 'white' if abs(matrix[i, j]) > vmax * 0.5 else 'black'\n",
    "                ax.text(\n",
    "                    j, i, f'{matrix[i, j]:.{precision}f}',\n",
    "                    ha='center', va='center', color=text_color, fontsize=8\n",
    "                )\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Column Index')\n",
    "    ax.set_ylabel('Row Index')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set tick marks\n",
    "    ax.set_xticks(np.arange(matrix.shape[1]))\n",
    "    ax.set_yticks(np.arange(matrix.shape[0]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8be35f",
   "metadata": {},
   "source": [
    "## **Question 1: A Single Attention Head**\n",
    "In this question, you'll implement an attention head.\n",
    "\n",
    "### Specifications:\n",
    "* The query-key matrices are provided already multiplied together (i.e., we provide as input $W_{QK} = W_Q^\\top W_K$, called `WQK`).\n",
    "* The output-value matrices are provided already multiplied together (i.e., we provide as input $W_{OV} = W_O^\\top W_V$, called `WOV`).\n",
    "* Attention inputs are also provided as a list of `d_model`-length vectors, called `attn_input`. The list elements correspond to positions in the context.\n",
    "* The desired outputs are the outputs the attention head produces at each position. This should be a list of `d_model`-length vectors of the same length as the input.\n",
    "* **Causal masking:** When implementing attention, mask out positions that come after the current position by setting their attention scores to negative infinity (e.g., `-1e9`) before applying softmax.\n",
    "* You should first convert the inputs to numpy arrays as a first step.\n",
    "\n",
    "### Note:\n",
    "* You should **not** use `np.softmax` when calculating the attention scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4db703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_attention_head(\n",
    "    attn_input: ArrayLike,\n",
    "    WQK: ArrayLike,\n",
    "    WOV: ArrayLike,\n",
    "    debug: bool = False,\n",
    ") -> NDArray[np.floating]:\n",
    "    \"\"\"\n",
    "    Implement a single attention head.\n",
    "\n",
    "    Args:\n",
    "        attn_input: Input to attention head, shape (seq_len, d_model)\n",
    "        WQK: Premultiplied query-key matrix, shape (d_model, d_model)\n",
    "        WOV: Premultiplied output-value matrix, shape (d_model, d_model)\n",
    "        debug: If True, plots heatmaps of scores, masked scores, attention weights, and XOV\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray - Output of attention head, shape (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # step 0: convert to numpy arrays\n",
    "    X = np.asarray(attn_input)\n",
    "    WQK = np.asarray(WQK)\n",
    "    WOV = np.asarray(WOV)\n",
    "\n",
    "    seq_len, d_model = X.shape\n",
    "\n",
    "    # TODO: compute the pre-attention scores by multiplying queries against keys\n",
    "    scores = ...\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(scores, title=\"Scores (Pre-Attention)\")\n",
    "\n",
    "    # TODO: apply a causal mask so tokens cannot attend to later positions\n",
    "    mask = ...\n",
    "    masked_scores = ...\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(masked_scores, title=\"Scores (After Causal Mask)\")\n",
    "\n",
    "    # TODO: apply softmax across each row\n",
    "    exp_scores = ...\n",
    "    attn_weights = ...\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(attn_weights, title=\"Attention Weights (After Softmax)\", cmap=\"Blues\")\n",
    "\n",
    "    # TODO: project values with WOV and combine them with the attention scores\n",
    "    projected_values = ...\n",
    "    out = ...\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(projected_values, title=\"Projected Values (XOV)\")\n",
    "        plot_weight_heatmap(out, title=\"Output\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8d292",
   "metadata": {},
   "source": [
    "### Test `single_attention_head` basic\n",
    "For additional debugging, set `debug=True` in the `single_attention_head` call to see the weights of matrices during attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_input = [\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "]\n",
    "WQK = [\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "]\n",
    "WOV = [\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "]\n",
    "\n",
    "expected_out = [\n",
    "    [1.0, 0.0],\n",
    "    [1.7310585786300048, 0.0],\n",
    "    [2.5752103826044417, 0.0],\n",
    "]\n",
    "\n",
    "out = single_attention_head(attn_input, WQK, WOV, debug=True).tolist()\n",
    "\n",
    "assert np.isclose(expected_out, out, atol=EPS).all(), f\"Failed:\\nExpected: {expected_out}\\nGot: {out}\"\n",
    "print(\"Test case passed ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6467d3",
   "metadata": {},
   "source": [
    "### Test `single_attention_head`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path('single_attention_head_test_cases.json').open() as f:\n",
    "    single_attention_head_test_cases = json.load(f)\n",
    "\n",
    "for test_case_id, (attn_input, WQK, WOV, expected_out) in single_attention_head_test_cases.items():\n",
    "    out = single_attention_head(attn_input, WQK, WOV).tolist()\n",
    "    case_num = test_case_id.split()[-1]\n",
    "    assert np.isclose(expected_out, out, atol=EPS).all(), (\n",
    "        f\"Test Case {case_num} Failed:\\nExpected: {expected_out}\\nGot: {out}\"\n",
    "    )\n",
    "\n",
    "print(\"All test cases passed ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ed8fe",
   "metadata": {},
   "source": [
    "## **Question 2: An Induction Copy Head**\n",
    "In this problem, you will combine a previous token head with a copying head.\n",
    "\n",
    "### Background:\n",
    "An induction head operates by predicting that previously-seen adjacencies in the sequence will be seen again. That is, it predicts `ab...a` will be followed by `b` (for any `a, b`).\n",
    "\n",
    "### Specifications:\n",
    "* #### Vocabulary and Embeddings\n",
    "  - Vocabulary size: 4\n",
    "  - Tokens: `a`, `b`, `c`, `d`\n",
    "  - Maximum sequence length: 5\n",
    "  - Embedding: 2-hot encoded with `d_model = 9`\n",
    "    - First 4 dimensions: 1-hot encoding of token vocabulary\n",
    "    - Next 5 dimensions: 1-hot encoding of position (0-4)\n",
    "  - The unembeddings are the same as the embeddings. The model produces an output vector of length 4 which encodes the logits on `a,b,c,d` respectively.\n",
    "* ### Induction Head Mechanism\n",
    "  - Your implementation should consist of two stages.\n",
    "     1. **Previous Token Head:** Identifies tokens that are directly adjacent to each other (using position information)\n",
    "     2. **Induction Head:** Takes the output from the previous token head and copies the token that follows matching patterns.\n",
    "\n",
    "     Together, these implement the pattern: `a,b,...,a -> b` for all `a, b`.\n",
    "* ### Previous Token Head\n",
    "  - Attend from each position to its direct predecessor\n",
    "  - Extract and pass forward the token identities\n",
    "  - Use `attention_strength` for the non-zero entries in the QK matrices.\n",
    "  - Use only 0's and 1's in the OV matrices.\n",
    "* ### Induction Head\n",
    "  - Take input from the previous token head summed with the original token embedding. To make life simpler:\n",
    "     - Exclude the positional embedding from this sum! That is, just sum the non-positional part of the input embedding with the output from the previous token head.\n",
    "     - Delete from this sum the first token position, since the previous token head does strange things (e.g., if you have an array of `[seq_len, d_model]`, it should become `[seq_len - 1, d_model]`).\n",
    "  - Copy the token that follows the previous instance of the present token.\n",
    "  - Use `attention_strength` for the non-zero entries in the QK matrices.\n",
    "  - Use only 0's and 1's in the OV matrices.\n",
    "\n",
    "### Output Format:\n",
    "* Return an array of 4 logits (one for each vocabulary token)\n",
    "* The output should be the prediction of the induction head on the last token in the sequence. The output should be just the output from that head (**not** the sum of residual stream with the head's output!).\n",
    "\n",
    "### Important Implementation Notes\n",
    "* **Causal masking:** When implementing attention, mask out positions that come after the current position by setting their attention scores to negative infinity (e.g., `-1e9`) before applying softmax.\n",
    "* **Matrix construction:** Use `attention_strength` for non-zero entries in QK matrices, and use `1.0` for non-zero entries in OV matrices (not `attention_strength`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7345e",
   "metadata": {},
   "source": [
    "### Example\n",
    "For the sequence `[a, b, c, d, a]`, the prediction should upweight `b`.\n",
    "\n",
    "#### Step-by-step for this example:\n",
    "**Previous Token Head** creates outputs where position $i$ contains information about the token at $i - 1$.\n",
    "\n",
    "After deleting position 0 and adding to token embeddings (without position info), we have representations that know both \"what token is here\" and \"what token came before\"\n",
    "\n",
    "**Induction Head** at the final position sees token `a` and looks for where else `a` appeared with its predecessor information\n",
    "\n",
    "It finds that position 0 had token `a` (with no predecessor)\n",
    "\n",
    "It copies what came after position 0, which is token `b`\n",
    "\n",
    "The output logits should thus have the highest value for token `b`\n",
    "\n",
    "### Note:\n",
    "* You should your use your previously implemented function `single_attention_head` for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6def4fe",
   "metadata": {},
   "source": [
    "### Hints\n",
    "- Build the previous token head and the copy head separately; test each intermediate tensor to sanity-check shapes.\n",
    "- Use the positional slots (dims 4-8) to aim queries at the previous position, and the token slots (dims 0-3) to copy token identities.\n",
    "- When adding the residual stream, drop position 0 so the induction head only sees valid previous-token pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induction_copy_head(\n",
    "    embeddings: ArrayLike,\n",
    "    attention_strength: float,\n",
    "    debug: bool = False,\n",
    ") -> NDArray[np.floating]:\n",
    "    \"\"\"\n",
    "    Implement an induction head mechanism combining a previous token head with a copy head.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Array of 2-hot encoding embeddings (token + position)\n",
    "        attention_strength: Attention strength multiplier\n",
    "        debug: If True, plots heatmaps of intermediate matrices\n",
    "\n",
    "    Returns:\n",
    "        Logits for the next token prediction (size 4)\n",
    "    \"\"\"\n",
    "    # Convert to numpy array and unpack dimensions\n",
    "    X = np.asarray(embeddings)\n",
    "    seq_len, d_model = X.shape\n",
    "    vocab_size = 4\n",
    "\n",
    "    # TODO: build the previous-token head matrices   \n",
    "    WQK_prev = np.zeros((d_model, d_model))\n",
    "    WOV_prev = np.zeros((d_model, d_model))\n",
    "    WQK_prev[...] = attention_strength * ...\n",
    "    WOV_prev[...] = ...\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(WQK_prev, title=\"WQK_prev (Projected Queries)\")\n",
    "        plot_weight_heatmap(WOV_prev, title=\"WOV_prev (Projected Values)\")\n",
    "\n",
    "    # TODO: run the previous-token head (you already wrote `single_attention_head`!)\n",
    "    prev_out = single_attention_head(...)\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(prev_out, title=\"prev_out (Previous Token Head Output)\", precision=6)\n",
    "\n",
    "    # TODO: add the residual stream's token information and drop position 0\n",
    "    prev_out[:seq_len, :vocab_size] += ...\n",
    "    prev_out = prev_out[1:, :]\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(prev_out, title=\"prev_out (With Residual Stream)\")\n",
    "\n",
    "    # TODO: build the copying head matrices\n",
    "    WQK_copy = np.zeros((d_model, d_model))\n",
    "    WOV_copy = np.zeros((d_model, d_model))\n",
    "    WQK_copy[...] = attention_strength * ...\n",
    "    WOV_copy[...] = ...\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(WQK_copy, title=\"WQK_copy (Projected Queries)\")\n",
    "        plot_weight_heatmap(WOV_copy, title=\"WOV_copy (Projected Values)\")\n",
    "\n",
    "    # TODO: run the copy head and read out logits for the last token\n",
    "    copy_out = single_attention_head(...)\n",
    "    out = copy_out[-1, :vocab_size]\n",
    "\n",
    "    if debug:\n",
    "        plot_weight_heatmap(copy_out, title=\"Copy Head Output\", precision=6)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e118ba",
   "metadata": {},
   "source": [
    "### Test `induction_copy_head` basic\n",
    "For additional debugging, set `debug=True` in the `induction_copy_head` call to see the weights of matrices during attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba65c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\n",
    "    [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "attention_strength = 10.0\n",
    "\n",
    "out = induction_copy_head(embeddings, attention_strength, debug=True).tolist()\n",
    "\n",
    "expected_out = [0.000045, 0.999864, 0.000045, 0.000045]\n",
    "\n",
    "assert np.isclose(expected_out, out, atol=EPS).all(), f\"Failed:\\nExpected: {expected_out}\\nGot: {out}\"\n",
    "print(\"Test case passed ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ccc3f",
   "metadata": {},
   "source": [
    "### Test `induction_copy_head`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9116b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path('induction_head_test_cases.json').open() as f:\n",
    "    induction_head_test_cases = json.load(f)\n",
    "\n",
    "for test_case_id, (embeddings, attention_strength, expected_out) in induction_head_test_cases.items():\n",
    "    out = induction_copy_head(embeddings, attention_strength).tolist()\n",
    "    case_num = test_case_id.split()[-1]\n",
    "    assert np.isclose(expected_out, out, atol=EPS).all(), (\n",
    "        f\"Test Case {case_num} Failed:\\nExpected: {expected_out}\\nGot: {out}\"\n",
    "    )\n",
    "\n",
    "print(\"All test cases passed ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718950ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182_hw10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
